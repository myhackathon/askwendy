{
  "cells": [
    {
      "cell_type": "code",
      "id": "jwxTtK2HEqrsVAPL90E0wLOv",
      "metadata": {
        "tags": [],
        "id": "jwxTtK2HEqrsVAPL90E0wLOv"
      },
      "source": [
        "pip install --upgrade google-api-python-client google-cloud-aiplatform langchain # 119.94"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x5_ysMVOFGxv",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1714750304511,
          "user_tz": -60,
          "elapsed": 35803,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "8ed4c399-5dac-44fc-ebf6-f0d4bc95b2d0"
      },
      "id": "x5_ysMVOFGxv",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gradio\n",
            "  Downloading gradio-4.28.3-py3-none-any.whl (12.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.2.2)\n",
            "Collecting fastapi (from gradio)\n",
            "  Downloading fastapi-0.111.0-py3-none-any.whl (91 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.3.2.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gradio-client==0.16.0 (from gradio)\n",
            "  Downloading gradio_client-0.16.0-py3-none-any.whl (314 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m314.4/314.4 kB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.27.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.20.3)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.4.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.3)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: numpy~=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.23.5)\n",
            "Collecting orjson~=3.0 (from gradio)\n",
            "  Downloading orjson-3.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (23.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.0.3)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (9.4.0)\n",
            "Collecting pydantic>=2.0 (from gradio)\n",
            "  Downloading pydantic-2.7.1-py3-none-any.whl (409 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m409.3/409.3 kB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Collecting python-multipart>=0.0.9 (from gradio)\n",
            "  Downloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.1)\n",
            "Collecting ruff>=0.2.2 (from gradio)\n",
            "  Downloading ruff-0.4.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m58.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting tomlkit==0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
            "Collecting typer<1.0,>=0.12 (from gradio)\n",
            "  Downloading typer-0.12.3-py3-none-any.whl (47 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.11.0)\n",
            "Requirement already satisfied: urllib3~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.0.7)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.29.0-py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==0.16.0->gradio) (2023.6.0)\n",
            "Collecting websockets<12.0,>=10.0 (from gradio-client==0.16.0->gradio)\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.0.5)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (3.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (3.13.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (4.66.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.6.0)\n",
            "Collecting pydantic-core==2.18.2 (from pydantic>=2.0->gradio)\n",
            "  Downloading pydantic_core-2.18.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m46.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
            "Collecting shellingham>=1.3.0 (from typer<1.0,>=0.12->gradio)\n",
            "  Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.7.1)\n",
            "Collecting starlette<0.38.0,>=0.37.2 (from fastapi->gradio)\n",
            "  Downloading starlette-0.37.2-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastapi-cli>=0.0.2 (from fastapi->gradio)\n",
            "  Downloading fastapi_cli-0.0.2-py3-none-any.whl (9.1 kB)\n",
            "Collecting ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 (from fastapi->gradio)\n",
            "  Downloading ujson-5.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.2/53.2 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting email_validator>=2.0.0 (from fastapi->gradio)\n",
            "  Downloading email_validator-2.1.1-py3-none-any.whl (30 kB)\n",
            "Collecting dnspython>=2.0.0 (from email_validator>=2.0.0->fastapi->gradio)\n",
            "  Downloading dnspython-2.6.1-py3-none-any.whl (307 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.34.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.18.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.16.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.24.1->gradio) (1.2.0)\n",
            "Collecting httptools>=0.5.0 (from uvicorn>=0.14.0->gradio)\n",
            "  Downloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.14.0->gradio) (1.0.0)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn>=0.14.0->gradio)\n",
            "  Downloading uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m81.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting watchfiles>=0.13 (from uvicorn>=0.14.0->gradio)\n",
            "  Downloading watchfiles-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m60.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->gradio) (3.3.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Building wheels for collected packages: ffmpy\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpy: filename=ffmpy-0.3.2-py3-none-any.whl size=5584 sha256=efc58ac5cd17397ed5b5db9fd5795f6b80096a2203bf4b6dd83f87fa6c4e6faf\n",
            "  Stored in directory: /root/.cache/pip/wheels/bd/65/9a/671fc6dcde07d4418df0c592f8df512b26d7a0029c2a23dd81\n",
            "Successfully built ffmpy\n",
            "Installing collected packages: pydub, ffmpy, websockets, uvloop, uvicorn, ujson, tomlkit, shellingham, semantic-version, ruff, python-multipart, pydantic-core, orjson, httptools, dnspython, aiofiles, watchfiles, starlette, pydantic, email_validator, typer, gradio-client, fastapi-cli, fastapi, gradio\n",
            "  Attempting uninstall: pydantic-core\n",
            "    Found existing installation: pydantic_core 2.18.1\n",
            "    Uninstalling pydantic_core-2.18.1:\n",
            "      Successfully uninstalled pydantic_core-2.18.1\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 1.10.15\n",
            "    Uninstalling pydantic-1.10.15:\n",
            "      Successfully uninstalled pydantic-1.10.15\n",
            "  Attempting uninstall: typer\n",
            "    Found existing installation: typer 0.9.4\n",
            "    Uninstalling typer-0.9.4:\n",
            "      Successfully uninstalled typer-0.9.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "spacy 3.7.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\n",
            "weasel 0.3.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aiofiles-23.2.1 dnspython-2.6.1 email_validator-2.1.1 fastapi-0.111.0 fastapi-cli-0.0.2 ffmpy-0.3.2 gradio-4.28.3 gradio-client-0.16.0 httptools-0.6.1 orjson-3.10.3 pydantic-2.7.1 pydantic-core-2.18.2 pydub-0.25.1 python-multipart-0.0.9 ruff-0.4.2 semantic-version-2.10.0 shellingham-1.5.4 starlette-0.37.2 tomlkit-0.12.0 typer-0.12.3 ujson-5.9.0 uvicorn-0.29.0 uvloop-0.19.0 watchfiles-0.21.0 websockets-11.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PROJECT_ID = \"turing-alcove-422022\"  # @param {type:\"string\"}\n",
        "REGION = \"us-central1\"  # @param {type: \"string\"}\n",
        "BUCKET_URI = \"gs://turing-alcove-422022-experiments-staging-bucket\"  # @param {type:\"string\"}\n",
        "EXPERIMENT_NAME = \"experiments-tutorial\"  # @param {type:\"string\"}"
      ],
      "metadata": {
        "id": "fznl9FyLRVj-",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1714743168702,
          "user_tz": -60,
          "elapsed": 3,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "fznl9FyLRVj-",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip3 install --upgrade --quiet google-cloud-aiplatform\n",
        "! pip3 install --upgrade --quiet google-cloud-aiplatform[autologging]\n",
        "\n",
        "! gcloud config set project {PROJECT_ID}\n",
        "\n",
        "# Only if your bucket doesn't already exist: Uncomment the following code to create your Cloud Storage bucket.\n",
        "# ! gsutil mb -l {REGION} -p {PROJECT_ID} {BUCKET_URI}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8dO93BVPRVj_",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1714602625684,
          "user_tz": -60,
          "elapsed": 78753,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "ae06a25f-c366-46d5-ba4b-4c273d2fb06d"
      },
      "id": "8dO93BVPRVj_",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.7/16.7 MB\u001b[0m \u001b[31m51.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.4/233.4 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.3/150.3 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m148.1/148.1 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m61.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.9/35.9 MB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m499.4/499.4 kB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m538.2/538.2 kB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m74.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.0.3, but you have pandas 1.5.3 which is incompatible.\n",
            "ipython-sql 0.5.0 requires sqlalchemy>=2.0, but you have sqlalchemy 1.4.52 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mUpdated property [core/project].\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Associate with an Experiment (Creates a new Experiment if it DNE)"
      ],
      "metadata": {
        "id": "Uh80np3zRVkA"
      },
      "id": "Uh80np3zRVkA"
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.cloud import aiplatform\n",
        "# aiplatform.init(\n",
        "#     project=PROJECT_ID,\n",
        "#     staging_bucket=BUCKET_URI,\n",
        "#     location=REGION,\n",
        "#     experiment=EXPERIMENT_NAME)\n",
        "\n",
        "# aiplatform.autolog()"
      ],
      "metadata": {
        "id": "KN4gvnynRVkM",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1714745858927,
          "user_tz": -60,
          "elapsed": 288,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "KN4gvnynRVkM",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install colab-xterm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7jruslr3YWK5",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1714688245578,
          "user_tz": -60,
          "elapsed": 18698,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "23657ad8-87af-457b-b506-3008b0612a5f"
      },
      "id": "7jruslr3YWK5",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting colab-xterm\n",
            "  Downloading colab_xterm-0.2.0-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.6/115.6 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: ptyprocess~=0.7.0 in /usr/local/lib/python3.10/dist-packages (from colab-xterm) (0.7.0)\n",
            "Requirement already satisfied: tornado>5.1 in /usr/local/lib/python3.10/dist-packages (from colab-xterm) (6.3.3)\n",
            "Installing collected packages: colab-xterm\n",
            "Successfully installed colab-xterm-0.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO(developer): Vertex AI SDK - uncomment below & run\n",
        "\n",
        "import vertexai\n",
        "from vertexai.generative_models import GenerativeModel, Part\n",
        "\n",
        "# Initialize Vertex AI\n",
        "vertexai.init(project=\"turing-alcove-422022\", location=\"us-central1\")\n",
        "# Load the model\n",
        "multimodal_model = GenerativeModel(model_name=\"gemini-1.0-pro-vision-001\")\n",
        "# Query the model\n",
        "response = multimodal_model.generate_content(\n",
        "    [\n",
        "        # Add an example image\n",
        "        Part.from_uri(\n",
        "            \"gs://generativeai-downloads/images/scones.jpg\", mime_type=\"image/jpeg\"\n",
        "        ),\n",
        "        # Add an example query\n",
        "        \"what is shown in this image?\",\n",
        "    ]\n",
        ")\n",
        "print(response)\n"
      ],
      "metadata": {
        "id": "o_iJDhogQowr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1714688424035,
          "user_tz": -60,
          "elapsed": 31381,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "eae3512e-ec7c-4884-eab2-8d61c4eb3d6e"
      },
      "id": "o_iJDhogQowr",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "candidates {\n",
            "  content {\n",
            "    role: \"model\"\n",
            "    parts {\n",
            "      text: \" The image shows a table with a cup of coffee, a bowl of blueberries, and a plate of scones with blueberries on it. There are also pink flowers on the table.\"\n",
            "    }\n",
            "  }\n",
            "  finish_reason: STOP\n",
            "  safety_ratings {\n",
            "    category: HARM_CATEGORY_HATE_SPEECH\n",
            "    probability: NEGLIGIBLE\n",
            "    probability_score: 0.027742892503738403\n",
            "    severity: HARM_SEVERITY_NEGLIGIBLE\n",
            "    severity_score: 0.07276838272809982\n",
            "  }\n",
            "  safety_ratings {\n",
            "    category: HARM_CATEGORY_DANGEROUS_CONTENT\n",
            "    probability: NEGLIGIBLE\n",
            "    probability_score: 0.026155617088079453\n",
            "    severity: HARM_SEVERITY_NEGLIGIBLE\n",
            "    severity_score: 0.07172112911939621\n",
            "  }\n",
            "  safety_ratings {\n",
            "    category: HARM_CATEGORY_HARASSMENT\n",
            "    probability: NEGLIGIBLE\n",
            "    probability_score: 0.043042849749326706\n",
            "    severity: HARM_SEVERITY_NEGLIGIBLE\n",
            "    severity_score: 0.03760863468050957\n",
            "  }\n",
            "  safety_ratings {\n",
            "    category: HARM_CATEGORY_SEXUALLY_EXPLICIT\n",
            "    probability: NEGLIGIBLE\n",
            "    probability_score: 0.08803311735391617\n",
            "    severity: HARM_SEVERITY_NEGLIGIBLE\n",
            "    severity_score: 0.09203285723924637\n",
            "  }\n",
            "}\n",
            "usage_metadata {\n",
            "  prompt_token_count: 265\n",
            "  candidates_token_count: 35\n",
            "  total_token_count: 300\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response.text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "OxmO3vdrZHs9",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1714688445181,
          "user_tz": -60,
          "elapsed": 26,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "466f6c6a-94f3-4f5e-f4fa-8abcf2f90348"
      },
      "id": "OxmO3vdrZHs9",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' The image shows a table with a cup of coffee, a bowl of blueberries, and a plate of scones with blueberries on it. There are also pink flowers on the table.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import vertexai\n",
        "\n",
        "project=\"turing-alcove-422022\"\n",
        "location=\"us-central1\"\n",
        "\n",
        "from vertexai.generative_models import GenerativeModel, ChatSession\n",
        "\n",
        "# TODO(developer): Update and un-comment below lines\n",
        "# project_id = \"PROJECT_ID\"\n",
        "# location = \"us-central1\"\n",
        "vertexai.init(project=project, location=location)\n",
        "model = GenerativeModel(model_name=\"gemini-1.0-pro-002\")\n",
        "chat = model.start_chat()\n",
        "\n",
        "def get_chat_response(chat: ChatSession, prompt: str) -> str:\n",
        "    text_response = []\n",
        "    responses = chat.send_message(prompt, stream=True)\n",
        "    for chunk in responses:\n",
        "        text_response.append(chunk.text)\n",
        "    return \"\".join(text_response)\n",
        "\n",
        "prompt = \"Hello.\"\n",
        "print(get_chat_response(chat, prompt))\n",
        "\n",
        "prompt = \"What are all the colors in a rainbow?\"\n",
        "print(get_chat_response(chat, prompt))\n",
        "\n",
        "prompt = \"Why does it appear when it rains?\"\n",
        "print(get_chat_response(chat, prompt))"
      ],
      "metadata": {
        "id": "fHMRt2ejZUT5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1714749596479,
          "user_tz": -60,
          "elapsed": 7106,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "64e55d49-be46-4b8d-992a-8dea83f35c13"
      },
      "id": "fHMRt2ejZUT5",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello! 👋 \n",
            "\n",
            "It's nice to meet you.  What can I do for you today? 😊\n",
            "A rainbow has **seven** main colors:\n",
            "\n",
            "1. **Red**\n",
            "2. **Orange**\n",
            "3. **Yellow**\n",
            "4. **Green**\n",
            "5. **Blue**\n",
            "6. **Indigo**\n",
            "7. **Violet**\n",
            "\n",
            "These colors appear in a specific order, with red on the outside and violet on the inside. This order is caused by the way sunlight interacts with water droplets in the air. \n",
            "\n",
            "Is there anything else you'd like to know about rainbows? 🌈\n",
            "A rainbow appears when sunlight shines through raindrops in the air. Here's how it happens:\n",
            "\n",
            "1. **Sunlight enters a raindrop:** The sunlight is made up of all the colors of the rainbow mixed together.\n",
            "2. **Refraction:** As the light enters the raindrop, it bends, or refracts, because it slows down slightly as it passes from air to water. Different colors of light bend at slightly different angles.\n",
            "3. **Reflection:** The light reaches the back of the raindrop and reflects off the surface.\n",
            "4. **More Refraction:** As the light leaves the raindrop, it bends again, separating the different colors further.\n",
            "5. **Rainbow:** The separated colors of light travel to our eyes, creating the beautiful arc of a rainbow.\n",
            "\n",
            "However, for a rainbow to appear, the sun needs to be behind you and the raindrops need to be in front of you. This is why rainbows are often seen after it has just rained and the sun is starting to peek through the clouds.\n",
            "\n",
            "Here are some additional interesting facts about rainbows:\n",
            "\n",
            "* The position of the sun in the sky determines the height of the rainbow. The higher the sun, the lower the rainbow will be.\n",
            "* Rainbows are actually circles, but we only see the top half because of the Earth's curvature.\n",
            "* You can sometimes see a double rainbow, which occurs when the light is reflected twice inside the raindrops.\n",
            "\n",
            "I hope this explanation helps! 🌈\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Assume you are an interviewer and I'm an interviewee as a female aged 18-20, and please feedback and criticise my answers\"\n",
        "print(get_chat_response(chat, prompt))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1NzGCmaMpTCf",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1714749606705,
          "user_tz": -60,
          "elapsed": 6866,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "6c16f0d2-b1f0-42b8-eb78-1c2041496a0c"
      },
      "id": "1NzGCmaMpTCf",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## Interview Simulation: Feedback and Critique \n",
            "\n",
            "**Please note:** I need more information to provide accurate and helpful feedback. To give you the best possible feedback, I need to know:\n",
            "\n",
            "* What position are you interviewing for?\n",
            "* What are the key skills and qualifications the position requires?\n",
            "* What specific questions did the interviewer ask you?\n",
            "* What answers did you give?\n",
            "\n",
            "Once I have this information, I can provide detailed feedback on your answers, including:\n",
            "\n",
            "* Whether your answers were clear, concise, and well-structured\n",
            "* How well you demonstrated your skills and qualifications\n",
            "* How you could improve your responses\n",
            "\n",
            "**Please provide additional information so I can give you the most helpful feedback possible.**\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import vertexai\n",
        "from vertexai.language_models import TextGenerationModel\n",
        "\n",
        "# TODO(developer): Update values for project_id, location & temperature\n",
        "vertexai.init(project=project, location=location)\n",
        "parameters = {\n",
        "    \"temperature\": 0,  # Temperature controls the degree of randomness in token selection.\n",
        "    \"max_output_tokens\": 256,  # Token limit determines the maximum amount of text output.\n",
        "    \"top_p\": 0.8,  # Tokens are selected from most probable to least until the sum of their probabilities equals the top_p value.\n",
        "    \"top_k\": 40,  # A top_k of 1 means the selected token is the most probable among all tokens.\n",
        "}\n",
        "\n",
        "model = TextGenerationModel.from_pretrained(\"text-bison@002\")\n",
        "response = model.predict(\n",
        "    \"Give me ten interview questions for the role of program manager.\",\n",
        "    **parameters,\n",
        ")\n",
        "print(f\"Response from Model: {response.text}\")\n",
        "\n",
        "response.text\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "id": "pIajxLk01HG5",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1714749613003,
          "user_tz": -60,
          "elapsed": 1498,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "7c3ccb0a-c73a-419b-b7f8-c3dab783b2be"
      },
      "id": "pIajxLk01HG5",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response from Model:  **1. Tell me about your experience as a program manager.**\n",
            "\n",
            "**2. What are your strengths and weaknesses as a program manager?**\n",
            "\n",
            "**3. What do you think are the most important qualities for a successful program manager?**\n",
            "\n",
            "**4. How do you manage stakeholder expectations?**\n",
            "\n",
            "**5. How do you deal with conflict and difficult situations?**\n",
            "\n",
            "**6. How do you measure the success of a program?**\n",
            "\n",
            "**7. What are your thoughts on Agile project management?**\n",
            "\n",
            "**8. What are your thoughts on risk management?**\n",
            "\n",
            "**9. What are your thoughts on change management?**\n",
            "\n",
            "**10. What are your salary expectations?**\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' **1. Tell me about your experience as a program manager.**\\n\\n**2. What are your strengths and weaknesses as a program manager?**\\n\\n**3. What do you think are the most important qualities for a successful program manager?**\\n\\n**4. How do you manage stakeholder expectations?**\\n\\n**5. How do you deal with conflict and difficult situations?**\\n\\n**6. How do you measure the success of a program?**\\n\\n**7. What are your thoughts on Agile project management?**\\n\\n**8. What are your thoughts on risk management?**\\n\\n**9. What are your thoughts on change management?**\\n\\n**10. What are your salary expectations?**'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "interview_prompts = [\n",
        "    \"Tell me about yourself.\",\n",
        "    \"What are your strengths and weaknesses?\",\n",
        "    \"Describe a time you faced a challenge and how you overcame it.\",\n",
        "    \"Why are you interested in this position?\",\n",
        "    \"What are your salary expectations?\",  # Add more questions as needed\n",
        "]"
      ],
      "metadata": {
        "id": "xFvczymREHOc"
      },
      "id": "xFvczymREHOc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Markdown"
      ],
      "metadata": {
        "id": "jMTqjnmbJf-i"
      },
      "id": "jMTqjnmbJf-i",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GenerativeModel(\n",
        "    model_name=\"gemini-1.5-pro-preview-0409\",\n",
        "    system_instruction=[\n",
        "        \"Assume you are an interviewer and I'm a female interviewee with age 18-30, and please feedback and criticise my answers\",\n",
        "        \"Your mission is to provide feedbacks\",\n",
        "    ],\n",
        ")\n",
        "\n",
        "prompt = \"\"\"\n",
        "User input: Question: Can you give me a feedback on \"`tell me about yourself\"\n",
        "Answer: My name is Irina and I work in Credit risk in HSBC. My career spans over more than 10 years. My latest role is leading on Basel 3.1 reforms implementation.\n",
        "\"\"\"\n",
        "\n",
        "contents = [prompt]\n",
        "\n",
        "response = model.generate_content(contents)\n",
        "\n",
        "Markdown(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "id": "LUpPoSfaC-J2",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1714751407087,
          "user_tz": -60,
          "elapsed": 7743,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "2dfd45d5-a0c8-4ceb-f994-00d881d4aa5e"
      },
      "id": "LUpPoSfaC-J2",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## Feedback on \"Tell Me About Yourself\" Answer\n\n**Strengths:**\n\n* **Clear and concise:** You provided your name, industry, and a brief overview of your experience.\n* **Highlights relevant experience:** Mentioning your leadership role in Basel 3.1 reforms implementation showcases your expertise and responsibility.\n\n**Areas for improvement:**\n\n* **Expand on your experience:** While mentioning 10 years of experience is good, elaborating on your career progression or key achievements would provide a richer picture. \n* **Connect to the specific role:** Tailor your response to the job you are interviewing for.  Highlight skills and experiences directly relevant to the position. \n* **Show personality and passion:** Consider sharing what motivates you in your career or what excites you about this opportunity. \n\n**Possible improvements:**\n\n* \"My name is Irina, and I'm a seasoned Credit Risk professional with over 10 years of experience, primarily in the banking sector. In my recent role at HSBC, I led the implementation of Basel 3.1 reforms, which significantly improved our risk management framework. I'm passionate about using my expertise to contribute to a company's financial stability and success, and I'm particularly interested in [mention something specific to the role or company].\"\n\n**Remember, the \"tell me about yourself\" question is an opportunity to make a strong first impression.  Make it count!** \n"
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "User input: Question: What are your strengths and weaknesses?\"\n",
        "Answer: My strength is I have a strong technical background and I have a good understanding of the financial industry. My weakness is I am not very good at communicating with people.\n",
        "I'm good at identifying big picture but I am not very detailed driven.\n",
        "\"\"\"\n",
        "\n",
        "contents = [prompt]\n",
        "\n",
        "response = model.generate_content(contents)\n",
        "\n",
        "Markdown(response.text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "id": "EVBd7LxfEubS",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1714751567993,
          "user_tz": -60,
          "elapsed": 8166,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "35d15aee-504e-4687-fdaa-7191b252712b"
      },
      "id": "EVBd7LxfEubS",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## Feedback on Your Answer: \n\n**Strengths:**\n\n*   **Good Start:** You've identified key strengths related to the job (technical background and financial industry understanding). \n*   **Specificity Needed:** Consider adding specific examples to showcase your strengths.  For example, instead of \"strong technical background,\" mention specific technologies or skills you've mastered. \n\n**Weaknesses:**\n\n*   **Honesty is Appreciated:**  It's good to be honest about your weaknesses, but try to reframe them positively. \n*   **Turn Negatives into Positives:** Instead of saying \"not very good at communicating,\" consider  \"I'm working on improving my communication skills by...\"  and provide specific actions you're taking, like joining a public speaking course or actively seeking feedback. \n*   **Balance is Key:** You mentioned two weaknesses that focus on detail-oriented tasks. It might be helpful to add another weakness that shows a different area for improvement to provide a more balanced perspective. \n\n**Additional Tips:**\n\n*   **Quantify your achievements:** Use numbers or data to demonstrate the impact of your work. \n*   **Tailor your answer to the specific job requirements:** Highlight the strengths most relevant to the position.\n*   **Practice your answer:**  This will help you deliver it confidently and concisely.\n\n**Remember,  the key is to present your weaknesses as areas for growth and demonstrate your commitment to self-improvement.**\n"
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "User input: Question: Why are you interested in this position?\"\n",
        "Answer: I have a strong technical background and good communication skills. I'd like to work in a challenging environment.\n",
        "\"\"\"\n",
        "\n",
        "contents = [prompt]\n",
        "\n",
        "response = model.generate_content(contents)\n",
        "\n",
        "Markdown(response.text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "n48D9mwJK3TO",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1714752272898,
          "user_tz": -60,
          "elapsed": 9188,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "1a74aeb5-504f-473d-d9d5-4b57767b14bf"
      },
      "id": "n48D9mwJK3TO",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## Feedback on \"Why are you interested in this position?\"\n\nWhile your answer touches on some relevant points, it lacks specifics and doesn't fully demonstrate your enthusiasm for the position or company. \n\n**Here's how you can improve:**\n\n* **Connect your skills to the role:** Instead of just mentioning \"strong technical background\" and \"good communication skills,\" explain how those skills will help you excel in this specific position. For example, mention specific technical skills relevant to the job description and how you've used them successfully in the past. \n* **Show your passion for the company/industry:** Research the company and mention what excites you about their mission, values, or recent projects. \n* **Highlight what makes you unique:** Share what sets you apart from other candidates. Do you have a unique combination of skills? Have you achieved something impressive in your field?\n* **Demonstrate your problem-solving ability:** Briefly mention a challenge you've overcome using your skills and how that experience makes you a good fit for this role. \n* **Tailor your answer to the specific company and role:** Avoid generic responses. Research the company and position to understand their needs and tailor your answer accordingly.\n\n**Example of a stronger answer:**\n\n\"I'm very interested in this position because it combines my passion for [industry/field] with my skills in [specific skills]. I've been particularly impressed by [company's achievement or project] and I believe my experience in [relevant experience] would allow me to contribute significantly to your team. I'm a highly motivated individual with a strong work ethic and I thrive in challenging environments where I can learn and grow.\" \n"
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dV2nhkM-K3WV"
      },
      "id": "dV2nhkM-K3WV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-LTRHmmSK3Z2"
      },
      "id": "-LTRHmmSK3Z2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr"
      ],
      "metadata": {
        "id": "U6Xx0y-kDd0q",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1714750347735,
          "user_tz": -60,
          "elapsed": 5712,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "U6Xx0y-kDd0q",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_sequence=\"\\nWendy:\"\n",
        "restart_sequence=\"\\nYou:\""
      ],
      "metadata": {
        "id": "byNh9Iq5FcIH",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1714750617522,
          "user_tz": -60,
          "elapsed": 351,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "byNh9Iq5FcIH",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def GenAI_chat(input, history):\n",
        "    history = history or []\n",
        "    s = list(sum(history, ()))\n",
        "    output = model.generate_content(s)\n",
        "    s.append(input,output)\n",
        "    inp = ' '.join(s)\n",
        "    return inp, history"
      ],
      "metadata": {
        "id": "UxT5D2H6FlD_",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1714750614671,
          "user_tz": -60,
          "elapsed": 357,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "UxT5D2H6FlD_",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "block = gr.Blocks()\n",
        "\n",
        "with block:\n",
        "    gr.Markdown(\n",
        "        \"\"\"\n",
        "        # Ask Wendy\n",
        "        \"\"\"\n",
        "    )\n",
        "    chatbot= gr.Chatbot()\n",
        "    message = gr.Textbox(placeholder=prompt)\n",
        "    state = gr.State()\n",
        "    submit = gr.Button(\"Send\")\n",
        "    submit.click(GenAI_chat, inputs=[gr.Textbox(placeholder=\"Enter your message...\"), gr.State()]  , outputs=[gr.Textbox(label=\"Response\"), gr.State()])\n",
        "\n",
        "block.launch()"
      ],
      "metadata": {
        "id": "wUij6RBlGD07",
        "executionInfo": {
          "status": "aborted",
          "timestamp": 1714751096059,
          "user_tz": -60,
          "elapsed": 51,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "wUij6RBlGD07",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "gr.ChatInterface(\n",
        "    model,\n",
        "    history,\n",
        "    start_sequence,\n",
        "    restart_sequence).launch()\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "id": "lFbVqFLCHoc1",
        "executionInfo": {
          "status": "error",
          "timestamp": 1714751346328,
          "user_tz": -60,
          "elapsed": 324,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "d08895ee-1e1f-4f50-b5b7-be3df052ed6c"
      },
      "id": "lFbVqFLCHoc1",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'history' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-04851586293e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m gr.ChatInterface(\n\u001b[1;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mstart_sequence\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     restart_sequence).launch()\n",
            "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " gr.Markdown(\n",
        "        \"\"\"\n",
        "        # Ask Wendy\n",
        "        \"\"\"\n",
        "    )\n",
        "    chatbot= gr.Chatbot()\n",
        "    message = gr.Textbox(placeholder=prompt)\n",
        "    state = gr.State()\n",
        "    submit = gr.Button(\"Send\")\n",
        "    submit.click(GenAI_chat, inputs=[gr.Textbox(placeholder=\"Enter your message...\"), gr.State()] , outputs=[gr.Textbox(label=\"Response\"), gr.State()])\n",
        "\n",
        "block.launch(inline=False)\n"
      ],
      "metadata": {
        "id": "4gr0fxZZJPX8"
      },
      "id": "4gr0fxZZJPX8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nKjpzqHXHCCQ"
      },
      "id": "nKjpzqHXHCCQ",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "name": "verchenguk (May 1, 2024, 11:12:17 PM)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}